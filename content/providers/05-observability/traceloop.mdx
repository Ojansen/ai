---
title: Traceloop
description: Monitoring and evaluating LLM applications with Traceloop
---

# Traceloop

Traceloop is a development platform for building reliable AI applications.
After integrating with the AI SDK, you can use Traceloop to trace, monitor, and experiment with LLM providers, prompts and flows. 

## Setup

### OpenTelemetry

Traceloop supports [AI SDK telemetry data](/docs/ai-sdk-core/telemetry) through [OpenTelemetry](https://opentelemetry.io/docs/).
You'll need to sign up at https://app.traceloop.com and get an API Key.

To use the AI SDK to send telemetry data to Traceloop, set these environment variables in your Next.js app's `.env` file:

```bash
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.traceloop.com
OTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer <Your API Key>"
```

You can then use the `experimental_telemetry` option to enable telemetry on supported AI SDK function calls:

```typescript
import { createOpenAI } from '@ai-sdk/openai';
import { generateText } from 'ai';

const openai = createOpenAI();

async function main() {
  const result = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: 'What is 2 + 2?',
    experimental_telemetry: {
      isEnabled: true,
      metadata: {
        query: 'weather',
        location: 'San Francisco',
      },
    },
  });
  console.log(result);
}

main();
```


## Resources

To see a demo of Traceloop, check out the [traceloop chatbot](https://www.traceloop.com/docs/demo)
Check out [Traceloop docs](https://www.traceloop.com/docs) to explore other capabilities of the platform.